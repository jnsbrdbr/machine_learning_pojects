{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnsbrdbr/machine_learning_pojects/blob/master/repo4/Transfomer(ViT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaorLuc4QNq7",
        "outputId": "fb9776c6-f694-46f1-9d3b-3f0af208daf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXF9pKY6QTS3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31zyCnmQd_5",
        "outputId": "c261c236-0a04-48d1-d7db-9d104a0c8595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28) - y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28) - y_test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "(x_train1, y_train1), (x_test1, y_test1) = keras.datasets.mnist.load_data()\n",
        "#(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train1.shape} - y_train shape: {y_train1.shape}\")\n",
        "print(f\"x_test shape: {x_test1.shape} - y_test shape: {y_test1.shape}\")\n",
        "#print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "#print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBRthmQbSXI1"
      },
      "outputs": [],
      "source": [
        "x_train=x_train1.reshape(60000,28,28,1)\n",
        "y_train=y_train1.reshape(60000,1)\n",
        "x_test=x_test1.reshape(10000,28,28,1)\n",
        "y_test=y_test1.reshape(10000,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PxhhX_-Atfl",
        "outputId": "7b759525-6251-49e8-a512-b30fda0137c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1) - y_train shape: (60000, 1)\n",
            "x_test shape: (10000, 28, 28, 1) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mzLYGSfNik0"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2lS1c5BbtUc"
      },
      "outputs": [],
      "source": [
        "#prevent overfitting\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)#x_train1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjlhM62bbtgM"
      },
      "outputs": [],
      "source": [
        "\n",
        "#normalize the contributio of these featurea\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch-TSA5fcaYm"
      },
      "outputs": [],
      "source": [
        "#turn 72 size picture to 6*6 patches\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "TTA46Hj5dHR6",
        "outputId": "b7796fb4-9b8e-4219-ed55-f8d0a6d7fc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches : (1, 144, 36)\n",
            "Patches per image: 144\n",
            "Elements per patch: 36\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 144 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image1 = x_train1[np.random.choice(range(x_train1.shape[0]))]\n",
        "\n",
        "plt.imshow(image1.astype(\"uint8\"),cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches : {patches.shape}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dusUud5P6JOp"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEtguRaX6tsL"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPTC6gBf6X2a",
        "outputId": "23585bca-0a95-439d-ec59-9a601b590e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "211/211 [==============================] - 3451s 16s/step - loss: 1.0340 - accuracy: 0.7118 - top-5-accuracy: 0.9580 - val_loss: 0.2175 - val_accuracy: 0.9370 - val_top-5-accuracy: 0.9965\n",
            "Epoch 2/100\n",
            "211/211 [==============================] - 3428s 16s/step - loss: 0.3860 - accuracy: 0.8782 - top-5-accuracy: 0.9934 - val_loss: 0.1157 - val_accuracy: 0.9698 - val_top-5-accuracy: 0.9983\n",
            "Epoch 3/100\n",
            "211/211 [==============================] - 3532s 17s/step - loss: 0.2501 - accuracy: 0.9231 - top-5-accuracy: 0.9970 - val_loss: 0.0741 - val_accuracy: 0.9825 - val_top-5-accuracy: 0.9985\n",
            "Epoch 4/100\n",
            "211/211 [==============================] - 3628s 17s/step - loss: 0.1989 - accuracy: 0.9394 - top-5-accuracy: 0.9976 - val_loss: 0.0777 - val_accuracy: 0.9803 - val_top-5-accuracy: 0.9985\n",
            "Epoch 5/100\n",
            "132/211 [=================>............] - ETA: 21:25 - loss: 0.1745 - accuracy: 0.9466 - top-5-accuracy: 0.9983"
          ]
        }
      ],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyCps+RVIgWCwCy9cJmaJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}